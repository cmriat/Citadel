"""
Script to convert HDF5 data to LeRobot dataset v2.1 format.

Usage:
    pixi run python test/lerobot_convert_v21.py \
        --hdf5-path /home/admin01/maoz/limx/assetes/episode_01_2025-12-19-16-13-07.hdf5 \
        --output-dir /home/admin01/maoz/limx/assetes/episode_0001 \
        --robot-type "limx Tron2" \
        --fps 30 \
        --task "Fold the laundry"
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple
import h5py
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import av
import tyro


def decode_jpeg_frames(hdf5_file, camera_name: str) -> np.ndarray:
    """è§£ç JPEGå‹ç¼©çš„å›¾åƒå¸§

    Args:
        hdf5_file: HDF5æ–‡ä»¶å¯¹è±¡
        camera_name: ç›¸æœºåç§° (cam_env, cam_left_wrist, cam_right_wrist)

    Returns:
        np.ndarray: [N, H, W, 3] RGB uint8å›¾åƒæ•°ç»„
    """
    from PIL import Image
    import io

    jpeg_frames = hdf5_file[f"images/{camera_name}/frames_jpeg"][:]
    decoded_frames = []

    for jpeg_bytes in jpeg_frames:
        # PILè§£ç JPEG
        image = Image.open(io.BytesIO(jpeg_bytes))
        # ç¡®ä¿RGBæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        # è½¬ä¸ºnumpyæ•°ç»„
        image_array = np.array(image, dtype=np.uint8)
        decoded_frames.append(image_array)

    return np.stack(decoded_frames, axis=0)


def reconstruct_joint_vector(hdf5_group, num_joints=6) -> np.ndarray:
    """ä»åˆ†æ•£çš„joint{1-6}_posé‡æ„ä¸ºå‘é‡

    Args:
        hdf5_group: HDF5ç»„å¯¹è±¡ (ä¾‹å¦‚ f["joints/left_slave"])
        num_joints: å…³èŠ‚æ•°é‡ï¼Œé»˜è®¤6

    Returns:
        np.ndarray: [N, num_joints] å…³èŠ‚ä½ç½®æ•°ç»„
    """
    joints = []
    for i in range(1, num_joints + 1):
        joint_key = f"joint{i}_pos"
        joints.append(hdf5_group[joint_key][:])
    return np.column_stack(joints)


def align_data_to_reference(ref_timestamps, data, data_timestamps, data_name, method='nearest'):
    """é€šç”¨çš„æ—¶é—´å¯¹é½å‡½æ•°

    Args:
        ref_timestamps: [N_ref] å‚è€ƒæ—¶é—´æˆ³
        data: [N_data, ...] å¾…å¯¹é½æ•°æ® (å¯ä»¥æ˜¯å›¾åƒæˆ–å…³èŠ‚)
        data_timestamps: [N_data] æ•°æ®æ—¶é—´æˆ³
        data_name: æ•°æ®åç§° (ç”¨äºæ—¥å¿—)
        method: å¯¹é½æ–¹æ³• - 'nearest' (æœ€è¿‘é‚») æˆ– 'linear' (çº¿æ€§æ’å€¼)

    Returns:
        aligned_data: [N_ref, ...] å¯¹é½åçš„æ•°æ®
    """
    if method == 'nearest':
        # æœ€è¿‘é‚»å¯¹é½ï¼ˆåŸæ–¹æ³•ï¼‰
        aligned_indices = []
        for ref_ts in ref_timestamps:
            closest_idx = np.argmin(np.abs(data_timestamps - ref_ts))
            aligned_indices.append(closest_idx)
        aligned_data = data[aligned_indices]

    elif method == 'linear':
        # çº¿æ€§æ’å€¼å¯¹é½
        from scipy.interpolate import interp1d

        # å¯¹äºå¤šç»´æ•°æ®ï¼Œéœ€è¦é€ç»´åº¦æ’å€¼
        if data.ndim == 1:
            interp_func = interp1d(
                data_timestamps, data,
                kind='linear',
                bounds_error=False,
                fill_value=(data[0], data[-1])  # è¾¹ç•Œä½¿ç”¨é¦–å°¾å€¼
            )
            aligned_data = interp_func(ref_timestamps)
        else:
            # å¤šç»´æ•°æ®ï¼šå¯¹æ¯ä¸ªç»´åº¦åˆ†åˆ«æ’å€¼
            aligned_data = np.zeros((len(ref_timestamps),) + data.shape[1:], dtype=data.dtype)
            for i in range(data.shape[1]):
                interp_func = interp1d(
                    data_timestamps, data[:, i],
                    kind='linear',
                    bounds_error=False,
                    fill_value=(data[0, i], data[-1, i])
                )
                aligned_data[:, i] = interp_func(ref_timestamps)
    else:
        raise ValueError(f"Unknown alignment method: {method}. Supported: 'nearest', 'linear'")

    # è®¡ç®—å¯¹é½è´¨é‡ï¼ˆåŸºäºæœ€è¿‘é‚»è¯¯å·®ï¼‰
    nearest_indices = [np.argmin(np.abs(data_timestamps - ts)) for ts in ref_timestamps]
    time_errors = np.abs(data_timestamps[nearest_indices] - ref_timestamps)
    print(f"  {data_name}: method={method}, å¹³å‡è¯¯å·®={np.mean(time_errors)/1e6:.2f}ms, æœ€å¤§è¯¯å·®={np.max(time_errors)/1e6:.2f}ms")

    return aligned_data


def detect_gap_segments(
    reference_timestamps: np.ndarray,
    cameras_info: Dict[str, np.ndarray],
    gap_factor: float = 5.0,
    min_segment_frames: int = 30
) -> List[np.ndarray]:
    """æ£€æµ‹æ‰€æœ‰ç›¸æœºçš„ä¸¥é‡è·³å¸§ï¼Œå°† reference_timestamps åˆ‡å‰²ä¸ºæœ‰æ•ˆç‰‡æ®µã€‚

    Args:
        reference_timestamps: åŸºå‡†æ—¶é—´æˆ³æ•°ç»„
        cameras_info: {ç›¸æœºå: æ—¶é—´æˆ³æ•°ç»„}
        gap_factor: å¸§é—´éš”è¶…è¿‡ median_interval * gap_factor è§†ä¸ºè·³å¸§
        min_segment_frames: æœ€å°æœ‰æ•ˆç‰‡æ®µå¸§æ•°

    Returns:
        æœ‰æ•ˆç‰‡æ®µçš„ reference_timestamps åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªè¿ç»­æ—¶é—´æ®µçš„æ—¶é—´æˆ³æ•°ç»„ï¼‰
        ç©ºåˆ—è¡¨è¡¨ç¤ºæ•´ä¸ª episode ä¸å¯ç”¨
    """
    # 1. æ”¶é›†æ‰€æœ‰ç›¸æœºçš„è·³å¸§åŒºé—´
    all_gap_intervals: List[Tuple[float, float]] = []

    for cam_name, cam_ts in cameras_info.items():
        if len(cam_ts) < 2:
            continue
        intervals = np.diff(cam_ts)
        median_interval = np.median(intervals)
        gap_threshold = median_interval * gap_factor

        gap_indices = np.where(intervals > gap_threshold)[0]
        for idx in gap_indices:
            gap_start_ts = cam_ts[idx]
            gap_end_ts = cam_ts[idx + 1]
            gap_duration_ms = (gap_end_ts - gap_start_ts) / 1e6
            print(f"  âš¡ {cam_name}: è·³å¸§ @ idx={idx}, é—´éš”={gap_duration_ms:.1f}ms "
                  f"(é˜ˆå€¼={gap_threshold/1e6:.1f}ms)")
            all_gap_intervals.append((gap_start_ts, gap_end_ts))

    # æ— è·³å¸§ â†’ è¿”å›å®Œæ•´æ—¶é—´æˆ³
    if not all_gap_intervals:
        print("  âœ… æ— ä¸¥é‡è·³å¸§ï¼Œä¿ç•™å®Œæ•´ episode")
        return [reference_timestamps]

    # 2. åˆå¹¶é‡å çš„è·³å¸§åŒºé—´ï¼ˆunionï¼‰
    all_gap_intervals.sort(key=lambda x: x[0])
    merged: List[Tuple[float, float]] = [all_gap_intervals[0]]
    for start, end in all_gap_intervals[1:]:
        if start <= merged[-1][1]:
            merged[-1] = (merged[-1][0], max(merged[-1][1], end))
        else:
            merged.append((start, end))

    # 3. ç”¨è·³å¸§åŒºé—´åˆ‡å‰² reference_timestamps
    #    åœ¨è·³å¸§åŒºé—´å†…çš„å‚è€ƒå¸§å…¨éƒ¨ä¸¢å¼ƒï¼ŒåŒºé—´å¤–çš„å¸§å½¢æˆè¿ç»­ç‰‡æ®µ
    segments: List[np.ndarray] = []
    current_mask = np.ones(len(reference_timestamps), dtype=bool)

    for gap_start, gap_end in merged:
        # æ ‡è®°è½åœ¨è·³å¸§åŒºé—´å†…çš„å‚è€ƒå¸§ä¸ºæ— æ•ˆ
        in_gap = (reference_timestamps >= gap_start) & (reference_timestamps <= gap_end)
        current_mask &= ~in_gap

    # ä»æœ‰æ•ˆå¸§ä¸­æå–è¿ç»­ç‰‡æ®µ
    valid_indices = np.where(current_mask)[0]
    if len(valid_indices) == 0:
        print("  âŒ æ‰€æœ‰å¸§å‡åœ¨è·³å¸§åŒºé—´å†…ï¼Œepisode ä¸å¯ç”¨")
        return []

    # æ£€æµ‹è¿ç»­ç´¢å¼•çš„æ–­ç‚¹ï¼ˆéè¿ç»­å¤„å³ä¸ºåˆ‡å‰²ç‚¹ï¼‰
    breaks = np.where(np.diff(valid_indices) > 1)[0] + 1
    index_groups = np.split(valid_indices, breaks)

    for group in index_groups:
        seg_ts = reference_timestamps[group]
        segments.append(seg_ts)

    # 4. è¿‡æ»¤æ‰å¸§æ•°ä¸è¶³çš„ç‰‡æ®µ
    valid_segments = []
    for i, seg in enumerate(segments):
        if len(seg) >= min_segment_frames:
            valid_segments.append(seg)
        else:
            print(f"  ğŸ—‘ï¸  ç‰‡æ®µ {i}: {len(seg)} å¸§ < {min_segment_frames} å¸§é˜ˆå€¼ï¼Œä¸¢å¼ƒ")

    # 5. æ‰“å°åˆ‡å‰²æŠ¥å‘Š
    print(f"\n  ğŸ“Š è·³å¸§åˆ‡å‰²æŠ¥å‘Š:")
    print(f"     æ£€æµ‹åˆ° {len(merged)} ä¸ªè·³å¸§åŒºé—´")
    print(f"     åˆ‡å‰²ä¸º {len(segments)} ä¸ªç‰‡æ®µï¼Œæœ‰æ•ˆ {len(valid_segments)} ä¸ª")
    for i, seg in enumerate(valid_segments):
        duration_s = (seg[-1] - seg[0]) / 1e9
        print(f"     ç‰‡æ®µ {i}: {len(seg)} å¸§, {duration_s:.2f}s")

    return valid_segments


def map_master_eef_to_slave_mapping(master_eef: np.ndarray, slave_mapping_stats: dict) -> np.ndarray:
    """å°†masterçš„eef_gripper_joint_posæ˜ å°„åˆ°slaveçš„gripper_mapping_controller_posèŒƒå›´

    Args:
        master_eef: [N, 1] masterçš„eefå¤¹çˆªåŸå§‹å€¼
        slave_mapping_stats: slave mappingçš„ç»Ÿè®¡ä¿¡æ¯ {'min': float, 'max': float}

    Returns:
        mapped_values: [N, 1] æ˜ å°„åçš„å€¼ï¼ŒèŒƒå›´ä¸slave_mappingä¸€è‡´

    æ³¨æ„ï¼šmaster_eefå’Œslave_mappingçš„ç‰©ç†æ„ä¹‰ç›¸åï¼š
        - master_eef: å°å€¼=æ‰“å¼€, å¤§å€¼=é—­åˆ
        - slave_mapping: å°å€¼=é—­åˆ, å¤§å€¼=æ‰“å¼€
        å› æ­¤éœ€è¦åå‘æ˜ å°„ï¼šmaster_min->target_max, master_max->target_min
    """
    # è®¡ç®—master_eefçš„èŒƒå›´ï¼ˆä½¿ç”¨å½“å‰æ•°æ®çš„å®é™…èŒƒå›´ï¼‰
    master_min = master_eef.min()
    master_max = master_eef.max()

    # è·å–ç›®æ ‡èŒƒå›´ï¼ˆslave_mappingï¼‰
    target_min = slave_mapping_stats['min']
    target_max = slave_mapping_stats['max']

    # åå‘çº¿æ€§æ˜ å°„ï¼šmaster_minï¼ˆæ‰“å¼€ï¼‰-> target_maxï¼ˆæ‰“å¼€ï¼‰, master_maxï¼ˆé—­åˆï¼‰-> target_minï¼ˆé—­åˆï¼‰
    # y = target_max - (x - x_min) / (x_max - x_min) * (y_max - y_min)
    mapped = target_max - (master_eef - master_min) / (master_max - master_min + 1e-8) * \
             (target_max - target_min)

    return mapped


def load_episode_v1_format(
    ep_path: Path,
    alignment_method: str = 'nearest',
    gap_factor: float = 5.0,
    min_segment_frames: int = 30
) -> List[Dict]:
    """åŠ è½½online_test_hdf5_v1æ ¼å¼çš„Episodeæ•°æ®ï¼Œæ”¯æŒè·³å¸§åˆ‡å‰²

    å½“æŸç›¸æœºå‡ºç°ä¸¥é‡è·³å¸§æ—¶ï¼Œåœ¨è·³å¸§å¤„åˆ‡å‰² episodeï¼Œä¿ç•™æ‰€æœ‰æœ‰æ•ˆç‰‡æ®µ
    ä½œä¸ºç‹¬ç«‹çš„ sub-episode è¾“å‡ºï¼Œæœ€å¤§åŒ–çœŸæœºæ•°æ®åˆ©ç”¨ç‡ã€‚

    æ•°æ®æ ¼å¼:
        images/cam_env/frames_jpeg - JPEGå‹ç¼©å›¾åƒ
        joints/{left|right}_{master|slave}/joint{1-6}_pos
        joints/{left|right}_{master|slave}/eef_gripper_joint_pos

    Args:
        ep_path: HDF5æ–‡ä»¶è·¯å¾„
        alignment_method: å¯¹é½æ–¹æ³• - 'nearest' (æœ€è¿‘é‚») æˆ– 'linear' (çº¿æ€§æ’å€¼)
        gap_factor: è·³å¸§åˆ¤å®šå€æ•°ï¼Œå¸§é—´éš” > æ­£å¸¸é—´éš” Ã— gap_factor è§†ä¸ºä¸¥é‡è·³å¸§
        min_segment_frames: æœ€å°æœ‰æ•ˆç‰‡æ®µå¸§æ•°ï¼Œä½äºæ­¤é˜ˆå€¼ä¸¢å¼ƒ

    Returns:
        æœ‰æ•ˆç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º:
        {
            'images_env': [N, H, W, 3] uint8,
            'images_left_wrist': [N, H, W, 3] uint8,
            'images_right_wrist': [N, H, W, 3] uint8,
            'state': [N, 14] float32,
            'action': [N, 14] float32
        }
        ç©ºåˆ—è¡¨è¡¨ç¤ºæ•´ä¸ª episode ä¸å¯ç”¨
    """
    with h5py.File(ep_path, "r") as f:
        # ========== 1. ç¡®å®šå‚è€ƒåŸºå‡†æ—¶é—´æˆ³ (æœ€å°‘å¸§æ•°ç›¸æœº) ==========
        cameras_info = {
            'cam_env': f["images/cam_env/timestamps"][:],
            'cam_left_wrist': f["images/cam_left_wrist/timestamps"][:],
            'cam_right_wrist': f["images/cam_right_wrist/timestamps"][:]
        }

        # æ‰¾åˆ°å¸§æ•°æœ€å°‘çš„ç›¸æœºä½œä¸ºåŸºå‡†
        min_camera = min(cameras_info, key=lambda k: len(cameras_info[k]))
        reference_timestamps = cameras_info[min_camera]
        N_frames_original = len(reference_timestamps)

        print(f"\nâ±ï¸  æ—¶é—´å¯¹é½åŸºå‡†: {min_camera} ({N_frames_original}å¸§)")

        # ========== 1.5 è‡ªé€‚åº”è¾¹ç•Œè£å‰ª ==========
        print("\nâœ‚ï¸  è‡ªé€‚åº”è¾¹ç•Œè£å‰ª:")

        # é¢„è¯»æ‰€æœ‰å…³èŠ‚æ•°æ®æºçš„æ—¶é—´æˆ³ä»¥è®¡ç®—è¾¹ç•Œ
        joint_groups = ["joints/left_slave", "joints/right_slave"]
        has_master = "left_master" in f["joints"]
        if has_master:
            joint_groups += ["joints/left_master", "joints/right_master"]

        all_joint_end_ts = []
        for grp in joint_groups:
            sec = f[f"{grp}/timestamp_sec"][:]
            nsec = f[f"{grp}/timestamp_nanosec"][:]
            ts = sec * 1e9 + nsec
            all_joint_end_ts.append(ts[-1])

        # ç”¨ left_slave è®¡ç®—å¤´å¸§æ—¶å»¶ï¼ˆä¼°ç®—å›¾åƒä¸å…³èŠ‚çš„å›ºæœ‰å»¶è¿Ÿï¼‰
        left_slave_ts = (f["joints/left_slave/timestamp_sec"][:] * 1e9
                         + f["joints/left_slave/timestamp_nanosec"][:])
        img_first_ts = reference_timestamps[0]
        joint_nearest_idx = np.argmin(np.abs(left_slave_ts - img_first_ts))
        joint_nearest_ts = left_slave_ts[joint_nearest_idx]
        head_delay_ns = img_first_ts - joint_nearest_ts
        head_delay_ms = head_delay_ns / 1e6

        print(f"  å¤´å¸§æ—¶å»¶: {head_delay_ms:+.2f} ms (å›¾åƒ{'æ™šäº' if head_delay_ns >= 0 else 'æ—©äº'}å…³èŠ‚)")

        # å–æ‰€æœ‰å…³èŠ‚æºä¸­æœ€æ—©ç»“æŸçš„æ—¶é—´æˆ³ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®æºéƒ½æœ‰è¦†ç›–
        joint_end_ts = min(all_joint_end_ts)
        print(f"  å…³èŠ‚æ•°æ®æº: {len(joint_groups)} ç»„, æœ€æ—©ç»“æŸ: {joint_end_ts/1e9:.3f}s")
        tolerance_end_ts = joint_end_ts + abs(head_delay_ns)  # ä½¿ç”¨ç»å¯¹å€¼ï¼Œç¡®ä¿å®¹å¿åº¦ä¸ºæ­£

        # è®¡ç®—åŸºå‡†ç›¸æœºçš„æœ‰æ•ˆå¸§æ©ç 
        valid_mask = reference_timestamps <= tolerance_end_ts

        # ç»Ÿè®¡è£å‰ªæƒ…å†µ
        trimmed_end = np.sum(reference_timestamps > tolerance_end_ts)

        if trimmed_end > 0:
            # è®¡ç®—è¶…å‡ºéƒ¨åˆ†çš„æ—¶é—´
            exceeded_frames = reference_timestamps[~valid_mask]
            max_exceed_ms = (exceeded_frames[-1] - tolerance_end_ts) / 1e6 if len(exceeded_frames) > 0 else 0

            print(f"  ç»“æŸè¾¹ç•Œå®¹å¿: joint_end + {abs(head_delay_ms):.2f}ms")
            print(f"  è£å‰ªå¸§æ•°: {trimmed_end} å¸§ (è¶…å‡ºå®¹å¿åº¦ {max_exceed_ms:.2f}ms)")

            # è£å‰ªåŸºå‡†ç›¸æœºçš„å‚è€ƒæ—¶é—´æˆ³
            reference_timestamps = reference_timestamps[valid_mask]
            N_frames = len(reference_timestamps)
            print(f"  ä¿ç•™å¸§æ•°: {N_frames} / {N_frames_original}")
        else:
            N_frames = N_frames_original
            print(f"  æ— éœ€è£å‰ªï¼Œæ‰€æœ‰ {N_frames_original} å¸§åœ¨å®¹å¿åº¦å†…")

        # ========== 1.6 è·³å¸§åˆ‡å‰² ==========
        print("\nğŸ” è·³å¸§æ£€æµ‹:")
        segments = detect_gap_segments(
            reference_timestamps, cameras_info,
            gap_factor=gap_factor,
            min_segment_frames=min_segment_frames
        )

        if not segments:
            print("\nâŒ Episode æ— æœ‰æ•ˆç‰‡æ®µ")
            return []

        # ========== 2. å…¨é‡è§£ç å›¾åƒï¼ˆåªåšä¸€æ¬¡ï¼‰ ==========
        print("\nğŸ“¸ å›¾åƒè§£ç :")
        images_env_raw = decode_jpeg_frames(f, "cam_env")
        print(f"  cam_env: {images_env_raw.shape}")
        images_left_raw = decode_jpeg_frames(f, "cam_left_wrist")
        print(f"  cam_left_wrist: {images_left_raw.shape}")
        images_right_raw = decode_jpeg_frames(f, "cam_right_wrist")
        print(f"  cam_right_wrist: {images_right_raw.shape}")

        # ========== 3. è¯»å–å…¨é‡å…³èŠ‚åŸå§‹æ•°æ®ï¼ˆåªåšä¸€æ¬¡ï¼‰ ==========
        # 3.1 left slave
        left_joints_raw = reconstruct_joint_vector(f["joints/left_slave"], 6)
        left_gripper_raw = f["joints/left_slave/gripper_mapping_controller_pos"][:][:, np.newaxis]
        left_joint_sec = f["joints/left_slave/timestamp_sec"][:]
        left_joint_nsec = f["joints/left_slave/timestamp_nanosec"][:]
        left_joint_timestamps = left_joint_sec * 1e9 + left_joint_nsec

        # 3.2 right slave
        right_joints_raw = reconstruct_joint_vector(f["joints/right_slave"], 6)
        right_gripper_raw = f["joints/right_slave/gripper_mapping_controller_pos"][:][:, np.newaxis]
        right_joint_sec = f["joints/right_slave/timestamp_sec"][:]
        right_joint_nsec = f["joints/right_slave/timestamp_nanosec"][:]
        right_joint_timestamps = right_joint_sec * 1e9 + right_joint_nsec

        # 3.3 masterï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if has_master:
            left_joints_cmd_raw = reconstruct_joint_vector(f["joints/left_master"], 6)
            left_gripper_cmd_raw = f["joints/left_master/eef_gripper_joint_pos"][:][:, np.newaxis]
            left_slave_mapping = f["joints/left_slave/gripper_mapping_controller_pos"][:]
            left_mapping_stats = {'min': left_slave_mapping.min(), 'max': left_slave_mapping.max()}
            left_cmd_sec = f["joints/left_master/timestamp_sec"][:]
            left_cmd_nsec = f["joints/left_master/timestamp_nanosec"][:]
            left_cmd_timestamps = left_cmd_sec * 1e9 + left_cmd_nsec

            right_joints_cmd_raw = reconstruct_joint_vector(f["joints/right_master"], 6)
            right_gripper_cmd_raw = f["joints/right_master/eef_gripper_joint_pos"][:][:, np.newaxis]
            right_slave_mapping = f["joints/right_slave/gripper_mapping_controller_pos"][:]
            right_mapping_stats = {'min': right_slave_mapping.min(), 'max': right_slave_mapping.max()}
            right_cmd_sec = f["joints/right_master/timestamp_sec"][:]
            right_cmd_nsec = f["joints/right_master/timestamp_nanosec"][:]
            right_cmd_timestamps = right_cmd_sec * 1e9 + right_cmd_nsec

        # ========== 4. å¯¹æ¯ä¸ª segment ç‹¬ç«‹æ‰§è¡Œå¯¹é½å’Œç»„è£… ==========
        results: List[Dict] = []

        for seg_idx, seg_timestamps in enumerate(segments):
            seg_label = f"ç‰‡æ®µ {seg_idx}" if len(segments) > 1 else "å®Œæ•´ episode"
            print(f"\n{'='*60}")
            print(f"ğŸ“¦ å¤„ç† {seg_label} ({len(seg_timestamps)} å¸§)")
            print(f"{'='*60}")

            # 4.1 å›¾åƒå¯¹é½
            print("\nğŸ“¸ å›¾åƒå¯¹é½:")
            seg_images_env = align_data_to_reference(
                seg_timestamps, images_env_raw, cameras_info['cam_env'],
                'cam_env', method='nearest'
            )
            seg_images_left = align_data_to_reference(
                seg_timestamps, images_left_raw, cameras_info['cam_left_wrist'],
                'cam_left_wrist', method='nearest'
            )
            seg_images_right = align_data_to_reference(
                seg_timestamps, images_right_raw, cameras_info['cam_right_wrist'],
                'cam_right_wrist', method='nearest'
            )

            # 4.2 å…³èŠ‚å¯¹é½
            print("\nğŸ¦¾ å…³èŠ‚å¯¹é½:")
            seg_left_joints = align_data_to_reference(
                seg_timestamps, left_joints_raw, left_joint_timestamps,
                'left_joints', method=alignment_method
            )
            seg_left_gripper = align_data_to_reference(
                seg_timestamps, left_gripper_raw, left_joint_timestamps,
                'left_gripper', method=alignment_method
            )
            seg_right_joints = align_data_to_reference(
                seg_timestamps, right_joints_raw, right_joint_timestamps,
                'right_joints', method=alignment_method
            )
            seg_right_gripper = align_data_to_reference(
                seg_timestamps, right_gripper_raw, right_joint_timestamps,
                'right_gripper', method=alignment_method
            )

            # 4.3 ç»„è£… State (14ç»´)
            state = np.concatenate([
                seg_left_joints,   # [N, 6]
                seg_left_gripper,  # [N, 1]
                seg_right_joints,  # [N, 6]
                seg_right_gripper  # [N, 1]
            ], axis=1).astype(np.float32)

            # 4.4 ç»„è£… Action (14ç»´)
            if has_master:
                print("\nğŸ® åŠ¨ä½œå¯¹é½:")
                seg_left_joints_cmd = align_data_to_reference(
                    seg_timestamps, left_joints_cmd_raw, left_cmd_timestamps,
                    'left_joints_cmd', method=alignment_method
                )
                seg_left_gripper_cmd_aligned = align_data_to_reference(
                    seg_timestamps, left_gripper_cmd_raw, left_cmd_timestamps,
                    'left_gripper_cmd', method=alignment_method
                )
                seg_right_joints_cmd = align_data_to_reference(
                    seg_timestamps, right_joints_cmd_raw, right_cmd_timestamps,
                    'right_joints_cmd', method=alignment_method
                )
                seg_right_gripper_cmd_aligned = align_data_to_reference(
                    seg_timestamps, right_gripper_cmd_raw, right_cmd_timestamps,
                    'right_gripper_cmd', method=alignment_method
                )

                seg_left_gripper_cmd = map_master_eef_to_slave_mapping(
                    seg_left_gripper_cmd_aligned, left_mapping_stats
                )
                seg_right_gripper_cmd = map_master_eef_to_slave_mapping(
                    seg_right_gripper_cmd_aligned, right_mapping_stats
                )

                print(f"  âœ“ å¤¹çˆªæ˜ å°„: master_eef -> slave_mapping èŒƒå›´")

                action = np.concatenate([
                    seg_left_joints_cmd,
                    seg_left_gripper_cmd,
                    seg_right_joints_cmd,
                    seg_right_gripper_cmd
                ], axis=1).astype(np.float32)

                print(f"  âœ… ä½¿ç”¨masteræ•°æ®ä½œä¸ºaction (å¤¹çˆªå·²æ˜ å°„)")
            else:
                action = state.copy()
                print(f"\n  âš ï¸  è­¦å‘Š: masteræ•°æ®ä¸å­˜åœ¨ï¼Œå¤åˆ¶slaveä½œä¸ºaction")

            results.append({
                'images_env': seg_images_env,
                'images_left_wrist': seg_images_left,
                'images_right_wrist': seg_images_right,
                'state': state,
                'action': action
            })

        # ========== 5. æ±‡æ€»æŠ¥å‘Š ==========
        total_frames = sum(len(r['state']) for r in results)
        print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ: {len(results)} ä¸ªç‰‡æ®µ, å…± {total_frames} å¸§\n")

        return results


def encode_video_frames(frames: np.ndarray, output_path: Path, fps: int = 30):
    """Encode RGB frame sequence to MP4 video."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    container = av.open(str(output_path), mode='w')
    stream = container.add_stream('h264', rate=fps)
    stream.width = frames.shape[2]   # W from [N, H, W, C]
    stream.height = frames.shape[1]  # H from [N, H, W, C]
    stream.pix_fmt = 'yuv420p'
    stream.options = {'crf': '23'}

    for frame in frames:
        av_frame = av.VideoFrame.from_ndarray(frame, format='rgb24')
        for packet in stream.encode(av_frame):
            container.mux(packet)

    # Flush stream
    for packet in stream.encode():
        container.mux(packet)

    container.close()


def create_episode_parquet(
    episode_data: Dict,
    output_path: Path,
    episode_index: int = 0,
    fps: int = 30
):
    """Create Parquet file for a single episode."""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    num_frames = len(episode_data['state'])

    # Create timestamp as float32 (seconds)
    timestamps = (np.arange(num_frames) / float(fps)).astype(np.float32).tolist()

    table = pa.table({
        'observation.state': episode_data['state'].tolist(),
        'action': episode_data['action'].tolist(),
        'timestamp': timestamps,
        'frame_index': np.arange(num_frames).tolist(),
        'episode_index': [episode_index] * num_frames,
        'index': np.arange(num_frames).tolist(),
        'task_index': [0] * num_frames,
    })

    pq.write_table(table, output_path)


def create_output_structure(output_dir: Path):
    """Create LeRobot v2.1 directory structure."""
    (output_dir / "meta").mkdir(parents=True, exist_ok=True)
    (output_dir / "data" / "chunk-000").mkdir(parents=True, exist_ok=True)

    for video_key in ["observation.images.cam_env",
                      "observation.images.cam_left_wrist",
                      "observation.images.cam_right_wrist"]:
        (output_dir / "videos" / "chunk-000" / video_key).mkdir(parents=True, exist_ok=True)


def generate_info_json(
    output_dir: Path,
    total_frames: int,
    total_episodes: int,
    fps: int,
    robot_type: str,
    dataset_name: str,
    image_height: int = 480,
    image_width: int = 640
):
    """Generate info.json metadata file in meta/ directory."""
    info = {
        "codebase_version": "v2.1",
        "robot_type": robot_type,
        "total_episodes": total_episodes,
        "total_frames": total_frames,
        "total_tasks": 1,
        "total_videos": 3 * total_episodes,
        "total_chunks": 1,
        "chunks_size": 1000,
        "fps": fps,
        "splits": {
            "train": f"0:{total_episodes}"
        },
        "data_path": "data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet",
        "video_path": "videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4",
        "features": {
            "observation.state": {
                "dtype": "float32",
                "shape": [14],
                "names": [
                    "left_joint1", "left_joint2", "left_joint3",
                    "left_joint4", "left_joint5", "left_joint6",
                    "left_gripper",
                    "right_joint1", "right_joint2", "right_joint3",
                    "right_joint4", "right_joint5", "right_joint6",
                    "right_gripper"
                ]
            },
            "action": {
                "dtype": "float32",
                "shape": [14],
                "names": [
                    "left_joint1", "left_joint2", "left_joint3",
                    "left_joint4", "left_joint5", "left_joint6",
                    "left_gripper",
                    "right_joint1", "right_joint2", "right_joint3",
                    "right_joint4", "right_joint5", "right_joint6",
                    "right_gripper"
                ]
            },
            "observation.images.cam_env": {
                "dtype": "video",
                "shape": [image_height, image_width, 3],
                "names": ["height", "width", "channels"],
                "info": {
                    "video.height": image_height,
                    "video.width": image_width,
                    "video.codec": "libx264",
                    "video.pix_fmt": "yuv420p",
                    "video.is_depth_map": False,
                    "video.fps": fps,
                    "video.channels": 3,
                    "has_audio": False
                }
            },
            "observation.images.cam_left_wrist": {
                "dtype": "video",
                "shape": [image_height, image_width, 3],
                "names": ["height", "width", "channels"],
                "info": {
                    "video.height": image_height,
                    "video.width": image_width,
                    "video.codec": "libx264",
                    "video.pix_fmt": "yuv420p",
                    "video.is_depth_map": False,
                    "video.fps": fps,
                    "video.channels": 3,
                    "has_audio": False
                }
            },
            "observation.images.cam_right_wrist": {
                "dtype": "video",
                "shape": [image_height, image_width, 3],
                "names": ["height", "width", "channels"],
                "info": {
                    "video.height": image_height,
                    "video.width": image_width,
                    "video.codec": "libx264",
                    "video.pix_fmt": "yuv420p",
                    "video.is_depth_map": False,
                    "video.fps": fps,
                    "video.channels": 3,
                    "has_audio": False
                }
            },
            "timestamp": {
                "dtype": "float32",
                "shape": [1],
                "names": None
            },
            "frame_index": {
                "dtype": "int64",
                "shape": [1],
                "names": None
            },
            "episode_index": {
                "dtype": "int64",
                "shape": [1],
                "names": None
            },
            "index": {
                "dtype": "int64",
                "shape": [1],
                "names": None
            },
            "task_index": {
                "dtype": "int64",
                "shape": [1],
                "names": None
            }
        },
        "info": {
            "dataset_name": dataset_name,
            "cameras": ["cam_left_wrist", "cam_right_wrist", "cam_env"],
            "alignment_strategy": "configurable",
            "action_space": "dual_arm_joint_position"
        }
    }

    with open(output_dir / "meta" / "info.json", "w") as f:
        json.dump(info, f, indent=2)


def generate_tasks_jsonl(output_dir: Path, task: str):
    """Generate tasks.jsonl metadata file in meta/ directory."""
    with open(output_dir / "meta" / "tasks.jsonl", "w") as f:
        f.write(json.dumps({"task_index": 0, "task": task}) + "\n")


def generate_episodes_jsonl(output_dir: Path, episodes_info: List[Dict], task: str):
    """Generate episodes.jsonl metadata file in meta/ directory.

    Args:
        output_dir: è¾“å‡ºç›®å½•
        episodes_info: [{'episode_index': int, 'num_frames': int}, ...]
        task: ä»»åŠ¡æè¿°
    """
    with open(output_dir / "meta" / "episodes.jsonl", "w") as f:
        for ep_info in episodes_info:
            f.write(json.dumps({
                "episode_index": ep_info['episode_index'],
                "tasks": [task],
                "length": ep_info['num_frames']
            }) + "\n")


def compute_episode_stats(episode_data: Dict, episode_index: int, fps: int) -> Dict:
    """Compute statistics for a single episode."""
    state = episode_data['state']
    action = episode_data['action']
    num_frames = len(state)

    # Compute timestamps in seconds
    timestamps = np.arange(num_frames) / float(fps)

    stats = {
        "episode_index": episode_index,
        "stats": {
            "observation.state": {
                "min": state.min(axis=0).tolist(),
                "max": state.max(axis=0).tolist(),
                "mean": state.mean(axis=0).tolist(),
                "std": state.std(axis=0).tolist(),
                "count": [num_frames]
            },
            "action": {
                "min": action.min(axis=0).tolist(),
                "max": action.max(axis=0).tolist(),
                "mean": action.mean(axis=0).tolist(),
                "std": action.std(axis=0).tolist(),
                "count": [num_frames]
            },
        }
    }

    # Compute image statistics (normalize to [0, 1])
    for cam_key in ['cam_env', 'cam_left_wrist', 'cam_right_wrist']:
        images_key = f"images_{cam_key.replace('cam_', '')}"
        images = episode_data[images_key].astype(np.float32) / 255.0

        # Sample 100 frames for image statistics (to reduce computation)
        num_samples = min(100, num_frames)
        sample_indices = np.linspace(0, num_frames - 1, num_samples, dtype=int)
        images_sampled = images[sample_indices]

        # Compute per-channel statistics
        min_vals = images_sampled.min(axis=(0, 1, 2))  # [C]
        max_vals = images_sampled.max(axis=(0, 1, 2))  # [C]
        mean_vals = images_sampled.mean(axis=(0, 1, 2))  # [C]
        std_vals = images_sampled.std(axis=(0, 1, 2))  # [C]

        stats["stats"][f"observation.images.{cam_key}"] = {
            "min": [[[float(v)]] for v in min_vals],
            "max": [[[float(v)]] for v in max_vals],
            "mean": [[[float(v)]] for v in mean_vals],
            "std": [[[float(v)]] for v in std_vals],
            "count": [num_samples]
        }

    # Compute timestamp statistics
    stats["stats"]["timestamp"] = {
        "min": [float(timestamps.min())],
        "max": [float(timestamps.max())],
        "mean": [float(timestamps.mean())],
        "std": [float(timestamps.std())],
        "count": [num_frames]
    }

    # Compute frame_index statistics
    frame_indices = np.arange(num_frames)
    stats["stats"]["frame_index"] = {
        "min": [int(frame_indices.min())],
        "max": [int(frame_indices.max())],
        "mean": [float(frame_indices.mean())],
        "std": [float(frame_indices.std())],
        "count": [num_frames]
    }

    # Compute episode_index statistics (all same value)
    stats["stats"]["episode_index"] = {
        "min": [episode_index],
        "max": [episode_index],
        "mean": [float(episode_index)],
        "std": [0.0],
        "count": [num_frames]
    }

    # Compute index statistics
    stats["stats"]["index"] = {
        "min": [int(frame_indices.min())],
        "max": [int(frame_indices.max())],
        "mean": [float(frame_indices.mean())],
        "std": [float(frame_indices.std())],
        "count": [num_frames]
    }

    # Compute task_index statistics (all 0 for single task)
    stats["stats"]["task_index"] = {
        "min": [0],
        "max": [0],
        "mean": [0.0],
        "std": [0.0],
        "count": [num_frames]
    }

    return stats


def generate_episodes_stats_jsonl(output_dir: Path, stats_list: list):
    """Generate episodes_stats.jsonl metadata file in meta/ directory."""
    with open(output_dir / "meta" / "episodes_stats.jsonl", "w") as f:
        for stats in stats_list:
            f.write(json.dumps(stats) + "\n")


def convert_hdf5_to_lerobot_v21(
    hdf5_path: Path,
    output_dir: Path,
    robot_type: str = "limx Tron2",
    fps: int = 30,
    task: str = "Fold the laundry",
    alignment_method: str = "nearest",
    gap_factor: float = 5.0,
    min_segment_frames: int = 30
):
    """Convert HDF5 episode to LeRobot v2.1 format.

    æ”¯æŒè·³å¸§åˆ‡å‰²ï¼šå½“æŸç›¸æœºå‡ºç°ä¸¥é‡è·³å¸§æ—¶ï¼Œåœ¨è·³å¸§å¤„åˆ‡å‰² episodeï¼Œ
    ä¿ç•™æ‰€æœ‰æœ‰æ•ˆç‰‡æ®µä½œä¸ºç‹¬ç«‹çš„ sub-episode è¾“å‡ºã€‚

    Args:
        hdf5_path: HDF5æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        robot_type: æœºå™¨äººç±»å‹
        fps: è§†é¢‘å¸§ç‡
        task: ä»»åŠ¡æè¿°
        alignment_method: å¯¹é½æ–¹æ³• - 'nearest' (æœ€è¿‘é‚») æˆ– 'linear' (çº¿æ€§æ’å€¼)
        gap_factor: è·³å¸§åˆ¤å®šå€æ•°ï¼Œå¸§é—´éš” > æ­£å¸¸é—´éš” Ã— gap_factor è§†ä¸ºä¸¥é‡è·³å¸§
        min_segment_frames: æœ€å°æœ‰æ•ˆç‰‡æ®µå¸§æ•°ï¼Œä½äºæ­¤é˜ˆå€¼ä¸¢å¼ƒ
    """
    dataset_name = output_dir.name
    print(f"Converting {hdf5_path} to LeRobot v2.1 format...")
    print(f"Output directory: {output_dir}")
    print(f"Dataset name: {dataset_name}")
    print(f"Alignment method: {alignment_method}")
    print(f"Gap detection: factor={gap_factor}, min_frames={min_segment_frames}")

    # 1. Create output directory structure
    create_output_structure(output_dir)

    # 2. Load HDF5 data (returns list of segments)
    print("\nLoading HDF5 data...")
    segments = load_episode_v1_format(
        hdf5_path,
        alignment_method=alignment_method,
        gap_factor=gap_factor,
        min_segment_frames=min_segment_frames
    )

    if not segments:
        print("\nâš ï¸  Episode æ— æœ‰æ•ˆç‰‡æ®µï¼Œè·³è¿‡")
        return

    num_episodes = len(segments)
    total_frames = sum(len(seg['state']) for seg in segments)
    print(f"\nLoaded {num_episodes} segment(s), {total_frames} total frames")

    # 3. ä¸ºæ¯ä¸ª segment è¾“å‡ºç‹¬ç«‹çš„ episode æ–‡ä»¶
    episodes_info = []
    all_stats = []

    for ep_idx, episode_data in enumerate(segments):
        num_frames = len(episode_data['state'])
        ep_tag = f"episode_{ep_idx:06d}"
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ è¾“å‡º {ep_tag} ({num_frames} å¸§)")
        print(f"{'='*60}")

        # 3.1 Encode videos
        print("  Encoding videos...")
        for cam_key in ['cam_env', 'cam_left_wrist', 'cam_right_wrist']:
            video_path = output_dir / "videos" / "chunk-000" / \
                         f"observation.images.{cam_key}" / f"{ep_tag}.mp4"

            images_key = f"images_{cam_key.replace('cam_', '')}"
            print(f"    {cam_key}... ", end="", flush=True)
            encode_video_frames(episode_data[images_key], video_path, fps)
            print(f"âœ“ {video_path.stat().st_size / 1024 / 1024:.1f} MB")

        # 3.2 Generate Parquet data file
        parquet_path = output_dir / "data" / "chunk-000" / f"{ep_tag}.parquet"
        create_episode_parquet(episode_data, parquet_path, episode_index=ep_idx, fps=fps)
        print(f"  âœ“ {parquet_path}")

        # 3.3 Compute episode statistics
        stats = compute_episode_stats(episode_data, episode_index=ep_idx, fps=fps)
        all_stats.append(stats)

        episodes_info.append({
            'episode_index': ep_idx,
            'num_frames': num_frames
        })

    # 4. Generate metadata files
    print("\nGenerating metadata files...")
    image_height = segments[0]['images_env'].shape[1]
    image_width = segments[0]['images_env'].shape[2]
    generate_info_json(output_dir, total_frames, num_episodes, fps, robot_type, dataset_name,
                       image_height=image_height, image_width=image_width)
    print("  âœ“ meta/info.json")

    generate_tasks_jsonl(output_dir, task)
    print("  âœ“ meta/tasks.jsonl")

    generate_episodes_jsonl(output_dir, episodes_info, task)
    print("  âœ“ meta/episodes.jsonl")

    generate_episodes_stats_jsonl(output_dir, all_stats)
    print("  âœ“ meta/episodes_stats.jsonl")

    print(f"\nâœ… Conversion complete!")
    print(f"   Output: {output_dir}")
    print(f"   Episodes: {num_episodes}")
    print(f"   Total frames: {total_frames}")
    for ep_info in episodes_info:
        print(f"     episode_{ep_info['episode_index']:06d}: {ep_info['num_frames']} frames")
    print(f"   Videos: {3 * num_episodes}")


def main(
    hdf5_path: Path,
    output_dir: Path,
    robot_type: str = "limx Tron2",
    fps: int = 30,
    task: str = "Fold the laundry",
    alignment_method: str = "nearest",
    gap_factor: float = 5.0,
    min_segment_frames: int = 30
):
    """Main entry point.

    Args:
        hdf5_path: HDF5æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        robot_type: æœºå™¨äººç±»å‹
        fps: è§†é¢‘å¸§ç‡
        task: ä»»åŠ¡æè¿°
        alignment_method: å…³èŠ‚å¯¹é½æ–¹æ³• - 'nearest' (æœ€è¿‘é‚») æˆ– 'linear' (çº¿æ€§æ’å€¼)
        gap_factor: è·³å¸§åˆ¤å®šå€æ•°ï¼Œå¸§é—´éš” > æ­£å¸¸é—´éš” Ã— gap_factor è§†ä¸ºä¸¥é‡è·³å¸§
        min_segment_frames: æœ€å°æœ‰æ•ˆç‰‡æ®µå¸§æ•°ï¼Œä½äºæ­¤é˜ˆå€¼ä¸¢å¼ƒ
    """
    convert_hdf5_to_lerobot_v21(
        hdf5_path, output_dir, robot_type, fps, task,
        alignment_method, gap_factor, min_segment_frames
    )


if __name__ == "__main__":
    tyro.cli(main)
